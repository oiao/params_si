{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is part of the supporting information for the paper  \n",
    "*ParAMS: Parameter Fitting for Atomistic and Molecular Models* (DOI: *123123*)  \n",
    "The full documentation can be found at https://www.scm.com/doc.trunk/params/index.html\n",
    "\n",
    "# SCC-DFTB repulsive potential parametrization\n",
    "\n",
    "Set num_processes to the number of processors on your machine. The DFTB calculations will be parallelized over that many cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParAMS Version used: 0.5.0\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from os.path    import join as opj\n",
    "from scm.params import *\n",
    "from scm.params import __version__ as paramsver\n",
    "from scm.plams import *\n",
    "\n",
    "num_processes = 8\n",
    "\n",
    "INDIR = '../dftbdata'\n",
    "if not os.path.exists(INDIR):\n",
    "    os.makedirs(INDIR)\n",
    "    \n",
    "\n",
    "\n",
    "print(f\"ParAMS Version used: {paramsver}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Define the job collection\n",
    "This adds lattice optimizations of the wurtzite and rocksalt polymorphs of ZnO to the job collection.\n",
    "\n",
    "For wurtzite, the elastic tensor is calculated. From the output, the bulk modulus can then be extracted.\n",
    "\n",
    "The job collection is stored in jobcollection.yml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Wurtzite ###\n",
      "  Atoms: \n",
      "    1        Zn      1.645000      0.949741      0.023375 \n",
      "    2        Zn      0.000000      1.899482      2.678375 \n",
      "    3         O      1.645000      0.949741      3.295375 \n",
      "    4         O      0.000000      1.899482      0.640375 \n",
      "  Lattice:\n",
      "        3.2900000000     0.0000000000     0.0000000000\n",
      "       -1.6450000000     2.8492235800     0.0000000000\n",
      "        0.0000000000     0.0000000000     5.3100000000\n",
      "\n",
      "### Rocksalt ###\n",
      "  Atoms: \n",
      "    1        Zn      2.170000      2.170000      2.170000 \n",
      "    2         O      0.000000      0.000000      0.000000 \n",
      "  Lattice:\n",
      "        0.0000000000     2.1700000000     2.1700000000\n",
      "        2.1700000000     0.0000000000     2.1700000000\n",
      "        2.1700000000     2.1700000000     0.0000000000\n",
      "\n",
      "### Job collection ###\n",
      "---\n",
      "ID: rocksalt_lattopt\n",
      "ReferenceEngineID: None\n",
      "AMSInput: |\n",
      "   Task GeometryOptimization\n",
      "   geometryoptimization\n",
      "     OptimizeLattice Yes\n",
      "   End\n",
      "   system\n",
      "     Atoms\n",
      "                Zn      2.1700000000      2.1700000000      2.1700000000 \n",
      "                 O      0.0000000000      0.0000000000      0.0000000000 \n",
      "     End\n",
      "     Lattice\n",
      "            0.0000000000     2.1700000000     2.1700000000\n",
      "            2.1700000000     0.0000000000     2.1700000000\n",
      "            2.1700000000     2.1700000000     0.0000000000\n",
      "     End\n",
      "   End\n",
      "---\n",
      "ID: wurtzite_lattopt\n",
      "ReferenceEngineID: None\n",
      "AMSInput: |\n",
      "   Task GeometryOptimization\n",
      "   geometryoptimization\n",
      "     OptimizeLattice Yes\n",
      "   End\n",
      "   properties\n",
      "     ElasticTensor Yes\n",
      "   End\n",
      "   system\n",
      "     Atoms\n",
      "                Zn      1.6450000000      0.9497411900      0.0233750800 \n",
      "                Zn      0.0000000000      1.8994823900      2.6783750800 \n",
      "                 O      1.6450000000      0.9497411900      3.2953749200 \n",
      "                 O      0.0000000000      1.8994823900      0.6403749200 \n",
      "     End\n",
      "     Lattice\n",
      "            3.2900000000     0.0000000000     0.0000000000\n",
      "           -1.6450000000     2.8492235800     0.0000000000\n",
      "            0.0000000000     0.0000000000     5.3100000000\n",
      "     End\n",
      "   End\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wurtzite, rocksalt = Molecule(opj(INDIR, 'w.xyz')), Molecule(opj(INDIR, 'rs.xyz')) \n",
    "print(\"### Wurtzite ###\")\n",
    "print(wurtzite)\n",
    "print(\"### Rocksalt ###\")\n",
    "print(rocksalt)\n",
    "\n",
    "w_opt_s = Settings()\n",
    "w_opt_s.input.ams.Task = 'GeometryOptimization'\n",
    "w_opt_s.input.ams.GeometryOptimization.OptimizeLattice = 'Yes'\n",
    "rs_opt_s = w_opt_s.copy()\n",
    "w_opt_s.input.ams.Properties.ElasticTensor = 'Yes' # to get bulk modulus of wurtzite\n",
    "\n",
    "jc = JobCollection()\n",
    "jc.add_entry('wurtzite_lattopt', JCEntry(w_opt_s, wurtzite))\n",
    "jc.add_entry('rocksalt_lattopt', JCEntry(rs_opt_s, rocksalt))\n",
    "print(\"### Job collection ###\")\n",
    "print(jc)\n",
    "jc.store(opj(INDIR, 'jobcollection.yml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Define the training set\n",
    "There are four target quantities\n",
    "* $a$ wurtzite lattice parameter, \n",
    "* $c$ wurtzite lattice parameter, \n",
    "* $B_0$ wurtzite bulk modulus, and\n",
    "* and $\\Delta E$ = relative energy between the wurtzite and rocksalt polymorphs (per ZnO formula unit).\n",
    "\n",
    "**If you set recalculate_reference_data to True**, the AMS BAND periodic DFT software will be used to run the reference jobs and calculate the reference data. Any engine, or combination of different engines in the Amsterdam Modeling Suite, can be used to seamlessly calculate the reference data, if the reference values are not known beforehand. NOTE: It may take many hours to calculate the reference data.\n",
    "\n",
    "**Otherwise**, DFT-calculated reference values are taken from https://doi.org/10.1021/jp404095x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:25:43] PLAMS working folder: /home/hellstrom/latex/params/params_si.git/trunk/dftbdata/band_reference_data.003\n",
      "[13:25:43] JOB wurtzite_lattopt STARTED\n",
      "[13:25:43] JOB wurtzite_lattopt RUNNING\n",
      "[23:29:26] JOB wurtzite_lattopt FINISHED\n",
      "[23:29:26] Job wurtzite_lattopt reported warnings. Please check the the output\n",
      "[23:29:26] JOB wurtzite_lattopt SUCCESSFUL\n",
      "[23:29:26] JOB rocksalt_lattopt STARTED\n",
      "[23:29:26] JOB rocksalt_lattopt RUNNING\n",
      "[01:58:39] JOB rocksalt_lattopt FINISHED\n",
      "[01:58:39] JOB rocksalt_lattopt SUCCESSFUL\n",
      "[01:58:39] PLAMS run finished. Goodbye\n",
      "### Training set ###\n",
      "---\n",
      "Expression: bulkmodulus(\"wurtzite_lattopt\")\n",
      "Weight: 1\n",
      "ReferenceValue: 120.298\n",
      "---\n",
      "Expression: lattice(\"wurtzite_lattopt\", 0)\n",
      "Weight: 1\n",
      "ReferenceValue: 3.3034906603720327\n",
      "---\n",
      "Expression: lattice(\"wurtzite_lattopt\", 2)\n",
      "Weight: 1\n",
      "ReferenceValue: 5.276393397180919\n",
      "---\n",
      "Expression: energy(\"wurtzite_lattopt\")/2.0-energy(\"rocksalt_lattopt\")\n",
      "Weight: 1\n",
      "ReferenceValue: -0.014083789325482599\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recalculate_reference_data = False\n",
    "training_set = DataSet()\n",
    "if recalculate_reference_data:\n",
    "    training_set.add_entry('bulkmodulus(\"wurtzite_lattopt\")', weight=1, reference=None) \n",
    "    training_set.add_entry('lattice(\"wurtzite_lattopt\", 0)', weight=1, reference=None) \n",
    "    training_set.add_entry('lattice(\"wurtzite_lattopt\", 2)', weight=1, reference=None)\n",
    "    training_set.add_entry('energy(\"wurtzite_lattopt\")/2.0-energy(\"rocksalt_lattopt\")', weight=1, reference=None)\n",
    "    band_settings = Settings()\n",
    "    band_settings.input.band.basis.type = 'TZP'\n",
    "    band_settings.input.band.kspace.quality = 'Normal' # Ideally should be \"Good\", but will be more expensive\n",
    "    band_settings.input.band.xc.libxc = 'PBE'\n",
    "    band_settings.runscript.nproc = num_processes\n",
    "    init(path=INDIR, folder='band_reference_data')\n",
    "    reference_results = jc.run(engine_settings=band_settings, use_pipe=False)\n",
    "    finish()\n",
    "    training_set.calculate_reference(reference_results)\n",
    "    training_set.store(opj(INDIR, 'trainingset_from_band_calculations.yml'))\n",
    "else:\n",
    "    training_set.add_entry('bulkmodulus(\"wurtzite_lattopt\")', weight=1, reference=129) # GPa\n",
    "    training_set.add_entry('lattice(\"wurtzite_lattopt\", 0)', weight=1, reference=3.29) # a, angstrom\n",
    "    training_set.add_entry('lattice(\"wurtzite_lattopt\", 2)', weight=1, reference=5.31) # c, angstrom\n",
    "    training_set.add_entry('energy(\"wurtzite_lattopt\")/2.0-energy(\"rocksalt_lattopt\")', weight=1, reference=-0.30/27.211)\n",
    "    training_set.store(opj(INDIR, 'trainingset_with_previous_reference_values.yml'))\n",
    "\n",
    "print(\"### Training set ###\")\n",
    "print(training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the settings for the parametrized DFTB engine. Here, we set the k-space quality to 'Good', which is important for lattice optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftb_s = Settings()\n",
    "dftb_s.input.dftb.kspace.quality = 'Good'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"parameter interface\" to the DFTB repulsive potential. \n",
    "\n",
    "Repulsive potentials are stored as splines towards the end of Slater-Koster (.skf) files.\n",
    "\n",
    "Here, we optimize only the Zn-O and O-Zn repulsive potentials (which must be identical).\n",
    "\n",
    "* Take electronic parameters and unchanged repulsive potentials (e.g. O-O.skf) from AMSHOME/atomicdata/DFTB/DFTB.org/znorg-0-1\n",
    "\n",
    "* Define an analytical repulsive function. Here, we choose a tapered double exponential of the form $V^{\\text{rep}}(r) = f^{\\text{cut}}(r)\\left[p_0\\exp(-p_1r) + p_2\\exp(-p_3r)\\right]$, where $p_0, p_1, p_2, p_3$ are the parameters to be fitted, and $f^\\text{cut}(r)$ is a smoothly decaying cutoff function decaying to 0 at $r = 5.67$ bohr.\n",
    "\n",
    "* r_range specifies for which distances to write the repulsive potential, and spline parameters, to the new O-Zn.skf and Zn-O.skf files.\n",
    "\n",
    "* Only optimize parameters for the O-Zn pair. Note: The Zn-O repulsive potential will be identical to the O-Zn one. When specifying active parameters for a DFTBSplineRepulsivePotentialParams, the elements must be ordered alphabetically.\n",
    "\n",
    "* Define initial values and allowed ranges for the parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Active parameters ###\n",
      "..................\n",
      "Name:     O-Zn:p0\n",
      "Value:    1.0\n",
      "Range:    (0.0, 10.0)\n",
      "Active:   True\n",
      "\n",
      "..................\n",
      "Name:     O-Zn:p1\n",
      "Value:    1.1\n",
      "Range:    (0.0, 10.0)\n",
      "Active:   True\n",
      "\n",
      "..................\n",
      "Name:     O-Zn:p2\n",
      "Value:    1.2\n",
      "Range:    (0.0, 10.0)\n",
      "Active:   True\n",
      "\n",
      "..................\n",
      "Name:     O-Zn:p3\n",
      "Value:    1.3\n",
      "Range:    (0.0, 10)\n",
      "Active:   True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interface = DFTBSplineRepulsivePotentialParams(\n",
    "    folder=opj(os.environ['AMSHOME'],'atomicdata', 'DFTB', 'DFTB.org', 'znorg-0-1'), \n",
    "    repulsive_function=TaperedDoubleExponential(cutoff=5.67), \n",
    "    r_range=np.arange(0., 5.87, 0.1), \n",
    "    other_settings=dftb_s\n",
    ")\n",
    "for p in interface:    \n",
    "    p.is_active = p.name.startswith('O-Zn:')\n",
    "\n",
    "print(\"### Active parameters ###\")\n",
    "interface.active.x = [1.0, 1.1, 1.2, 1.3] # initial values\n",
    "interface.active.range = [ (0.,10.), (0.,10.), (0.,10.), (0.,10) ]\n",
    "for p in interface.active:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run the optimization\n",
    "* Specify a Nelder-Mead optimizer from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'ZnO_repulsive_opt' already exists. Will use 'ZnO_repulsive_opt.003' instead.\n",
      "Optimization() Instance Settings:\n",
      "=================================\n",
      "Title:                             ZnO_repulsive_opt.003\n",
      "Workdir:                           /home/hellstrom/latex/params/params_si.git/trunk/notebooks/ZnO_repulsive_opt.003\n",
      "JobCollection size:                2\n",
      "Interface:                         DFTBSplineRepulsivePotentialParams\n",
      "Active parameters:                 4\n",
      "Optimizer:                         Scipy\n",
      "Parallelism:                       ParallelLevels(optimizations=1, parametervectors=1, jobs=1, processes=8, threads=1)\n",
      "Verbose:                           True\n",
      "Callbacks:                         Logger\n",
      "                                   TimePerEval\n",
      "\n",
      "Evaluators:\n",
      "-----------\n",
      "Name:                              trainingset (_LossEvaluator)\n",
      "Loss:                              SSE\n",
      "Evaluation frequency:              1\n",
      "\n",
      "Data Set entries:                  4\n",
      "Data Set jobs:                     2\n",
      "Batch size:                        None\n",
      "\n",
      "Use PIPE:                          False\n",
      "---\n",
      "===\n",
      "[2020-12-17 01:58:39] Starting parameter optimization.\n",
      "[2020-12-17 02:00:03] Initial loss = 1.236e+02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/site-packages/scipy/optimize/_minimize.py:517: RuntimeWarning: Method Nelder-Mead cannot handle constraints nor bounds.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-17 02:01:27] Best trainingset loss = 1.236e+02\n",
      "[2020-12-17 02:01:27] Step 1, trainingset loss = 123.563, first 4 params = 1.00 1.10 1.20 1.30\n",
      "[2020-12-17 02:02:47] Step 2, trainingset loss = 150.394, first 4 params = 0.80 1.10 1.20 1.30\n",
      "[2020-12-17 02:04:39] Best trainingset loss = 7.974e+01\n",
      "[2020-12-17 02:04:39] Step 3, trainingset loss = 79.742, first 4 params = 1.00 0.90 1.20 1.30\n",
      "[2020-12-17 02:06:07] Step 4, trainingset loss = 132.506, first 4 params = 1.00 1.10 1.01 1.30\n",
      "[2020-12-17 02:07:34] Step 5, trainingset loss = 81.254, first 4 params = 1.00 1.10 1.20 1.11\n",
      "[2020-12-17 02:09:25] Step 6, trainingset loss = 80.239, first 4 params = 1.20 1.00 1.10 1.21\n",
      "[2020-12-17 02:11:18] Step 7, trainingset loss = 102.324, first 4 params = 1.10 0.95 1.34 1.16\n",
      "[2020-12-17 02:13:13] Step 8, trainingset loss = 216.302, first 4 params = 1.15 0.88 1.22 1.09\n",
      "[2020-12-17 02:14:34] Step 9, trainingset loss = 86.918, first 4 params = 1.04 1.05 1.21 1.25\n",
      "[2020-12-17 02:15:55] Step 10, trainingset loss = 114.083, first 4 params = 1.02 1.07 1.01 1.27\n",
      "[2020-12-17 02:15:55] Time per f-evaluation (trainingset): 0:01:36.435462\n",
      "[2020-12-17 02:17:47] Step 11, trainingset loss = 82.879, first 4 params = 1.08 0.98 1.26 1.19\n",
      "[2020-12-17 02:19:43] Step 12, trainingset loss = 95.021, first 4 params = 1.10 0.95 1.18 1.16\n",
      "[2020-12-17 02:21:13] Step 13, trainingset loss = 80.506, first 4 params = 1.05 1.02 1.20 1.23\n",
      "[2020-12-17 02:22:37] Step 14, trainingset loss = 85.391, first 4 params = 1.05 1.03 1.09 1.23\n",
      "[2020-12-17 02:24:23] Best trainingset loss = 7.940e+01\n",
      "[2020-12-17 02:24:23] Step 15, trainingset loss = 79.399, first 4 params = 1.07 1.00 1.22 1.20\n",
      "[2020-12-17 02:26:15] Step 16, trainingset loss = 111.040, first 4 params = 1.16 0.86 1.16 1.35\n",
      "[2020-12-17 02:27:58] Step 17, trainingset loss = 79.882, first 4 params = 1.04 1.04 1.19 1.17\n",
      "[2020-12-17 02:29:54] Step 18, trainingset loss = 84.624, first 4 params = 1.10 0.95 1.16 1.22\n",
      "[2020-12-17 02:31:40] Best trainingset loss = 7.880e+01\n",
      "[2020-12-17 02:31:40] Step 19, trainingset loss = 78.805, first 4 params = 1.07 1.00 1.19 1.22\n",
      "[2020-12-17 02:33:19] Step 20, trainingset loss = 79.277, first 4 params = 0.89 0.97 1.29 1.24\n",
      "[2020-12-17 02:33:19] Time per f-evaluation (trainingset): 0:01:40.622784\n",
      "[2020-12-17 02:35:12] Step 21, trainingset loss = 80.056, first 4 params = 0.97 0.90 1.26 1.31\n",
      "[2020-12-17 02:36:59] Best trainingset loss = 7.880e+01\n",
      "[2020-12-17 02:36:59] Step 22, trainingset loss = 78.799, first 4 params = 1.02 1.00 1.21 1.21\n",
      "[2020-12-17 02:38:45] Step 23, trainingset loss = 80.319, first 4 params = 1.03 1.08 1.25 1.14\n",
      "[2020-12-17 02:40:31] Best trainingset loss = 7.788e+01\n",
      "[2020-12-17 02:40:31] Step 24, trainingset loss = 77.884, first 4 params = 1.01 0.95 1.21 1.26\n",
      "[2020-12-17 02:41:57] Step 25, trainingset loss = 79.928, first 4 params = 0.92 0.97 1.23 1.26\n",
      "[2020-12-17 02:43:44] Step 26, trainingset loss = 78.546, first 4 params = 1.03 0.99 1.22 1.22\n",
      "[2020-12-17 02:45:32] Step 27, trainingset loss = 79.552, first 4 params = 1.18 1.00 1.12 1.21\n",
      "[2020-12-17 02:47:16] Step 28, trainingset loss = 78.475, first 4 params = 0.96 0.98 1.25 1.23\n",
      "[2020-12-17 02:49:05] Best trainingset loss = 7.774e+01\n",
      "[2020-12-17 02:49:05] Step 29, trainingset loss = 77.744, first 4 params = 0.95 0.96 1.26 1.24\n",
      "[2020-12-17 02:50:54] Best trainingset loss = 7.739e+01\n",
      "[2020-12-17 02:50:54] Step 30, trainingset loss = 77.388, first 4 params = 0.89 0.93 1.29 1.24\n",
      "[2020-12-17 02:50:54] Time per f-evaluation (trainingset): 0:01:42.296642\n",
      "[2020-12-17 02:52:42] Step 31, trainingset loss = 77.599, first 4 params = 0.92 0.92 1.28 1.27\n",
      "[2020-12-17 02:54:28] Best trainingset loss = 7.638e+01\n",
      "[2020-12-17 02:54:28] Step 32, trainingset loss = 76.384, first 4 params = 0.85 0.90 1.30 1.29\n",
      "[2020-12-17 02:56:18] Best trainingset loss = 7.495e+01\n",
      "[2020-12-17 02:56:18] Step 33, trainingset loss = 74.948, first 4 params = 0.76 0.86 1.34 1.32\n",
      "[2020-12-17 02:58:12] Step 34, trainingset loss = 78.229, first 4 params = 0.83 0.85 1.31 1.31\n",
      "[2020-12-17 03:00:02] Step 35, trainingset loss = 77.200, first 4 params = 0.86 0.88 1.30 1.29\n",
      "[2020-12-17 03:01:51] Step 36, trainingset loss = 74.996, first 4 params = 0.71 0.85 1.39 1.30\n",
      "[2020-12-17 03:03:37] Best trainingset loss = 7.469e+01\n",
      "[2020-12-17 03:03:37] Step 37, trainingset loss = 74.691, first 4 params = 0.69 0.84 1.38 1.31\n",
      "[2020-12-17 03:05:17] Step 38, trainingset loss = 75.102, first 4 params = 0.57 0.80 1.42 1.33\n",
      "[2020-12-17 03:07:04] Best trainingset loss = 7.247e+01\n",
      "[2020-12-17 03:07:04] Step 39, trainingset loss = 72.472, first 4 params = 0.62 0.78 1.41 1.37\n",
      "[2020-12-17 03:08:54] Best trainingset loss = 7.020e+01\n",
      "[2020-12-17 03:08:54] Step 40, trainingset loss = 70.199, first 4 params = 0.49 0.71 1.46 1.43\n",
      "[2020-12-17 03:08:54] Time per f-evaluation (trainingset): 0:01:43.761581\n",
      "[2020-12-17 03:10:18] Step 41, trainingset loss = 78.106, first 4 params = 0.47 0.74 1.49 1.39\n",
      "[2020-12-17 03:12:09] Step 42, trainingset loss = 75.235, first 4 params = 0.76 0.85 1.34 1.32\n",
      "[2020-12-17 03:13:53] Step 43, trainingset loss = 73.686, first 4 params = 0.56 0.78 1.44 1.36\n",
      "[2020-12-17 03:15:40] Step 44, trainingset loss = 71.544, first 4 params = 0.55 0.74 1.42 1.41\n",
      "[2020-12-17 03:17:07] Step 45, trainingset loss = 79.824, first 4 params = 0.38 0.68 1.51 1.44\n",
      "[2020-12-17 03:18:57] Step 46, trainingset loss = 73.478, first 4 params = 0.67 0.81 1.38 1.35\n",
      "[2020-12-17 03:20:27] Step 47, trainingset loss = 70.869, first 4 params = 0.45 0.68 1.47 1.47\n",
      "[2020-12-17 03:22:16] Best trainingset loss = 6.857e+01\n",
      "[2020-12-17 03:22:16] Step 48, trainingset loss = 68.568, first 4 params = 0.51 0.69 1.43 1.47\n",
      "[2020-12-17 03:24:06] Best trainingset loss = 6.701e+01\n",
      "[2020-12-17 03:24:06] Step 49, trainingset loss = 67.015, first 4 params = 0.48 0.65 1.42 1.52\n",
      "[2020-12-17 03:25:33] Step 50, trainingset loss = 75.897, first 4 params = 0.32 0.58 1.51 1.56\n",
      "[2020-12-17 03:25:33] Time per f-evaluation (trainingset): 0:01:42.983355\n",
      "[2020-12-17 03:27:20] Step 51, trainingset loss = 71.292, first 4 params = 0.58 0.75 1.41 1.40\n",
      "[2020-12-17 03:29:09] Best trainingset loss = 6.687e+01\n",
      "[2020-12-17 03:29:09] Step 52, trainingset loss = 66.871, first 4 params = 0.46 0.65 1.47 1.50\n",
      "[2020-12-17 03:31:00] Best trainingset loss = 6.473e+01\n",
      "[2020-12-17 03:31:00] Step 53, trainingset loss = 64.732, first 4 params = 0.41 0.61 1.49 1.54\n",
      "[2020-12-17 03:32:29] Step 54, trainingset loss = 66.216, first 4 params = 0.34 0.57 1.51 1.58\n",
      "[2020-12-17 03:34:24] Step 55, trainingset loss = 65.375, first 4 params = 0.42 0.59 1.47 1.56\n",
      "[2020-12-17 03:36:25] Best trainingset loss = 6.109e+01\n",
      "[2020-12-17 03:36:25] Step 56, trainingset loss = 61.089, first 4 params = 0.33 0.50 1.49 1.67\n",
      "[2020-12-17 03:38:18] Best trainingset loss = 5.679e+01\n",
      "[2020-12-17 03:38:18] Step 57, trainingset loss = 56.787, first 4 params = 0.25 0.39 1.50 1.79\n",
      "[2020-12-17 03:39:48] Step 58, trainingset loss = 65.313, first 4 params = 0.22 0.43 1.57 1.72\n",
      "[2020-12-17 03:41:41] Step 59, trainingset loss = 69.889, first 4 params = 0.31 0.44 1.50 1.73\n",
      "[2020-12-17 03:43:31] Step 60, trainingset loss = 61.828, first 4 params = 0.33 0.54 1.51 1.61\n",
      "[2020-12-17 03:43:31] Time per f-evaluation (trainingset): 0:01:43.805097\n",
      "[2020-12-17 03:44:52] Step 61, trainingset loss = 73.346, first 4 params = 0.19 0.39 1.56 1.77\n",
      "[2020-12-17 03:46:45] Step 62, trainingset loss = 62.288, first 4 params = 0.36 0.54 1.49 1.61\n",
      "[2020-12-17 03:48:37] Step 63, trainingset loss = 66.581, first 4 params = 0.45 0.61 1.43 1.56\n",
      "[2020-12-17 03:50:27] Step 64, trainingset loss = 58.867, first 4 params = 0.28 0.47 1.53 1.68\n",
      "[2020-12-17 03:52:12] Best trainingset loss = 5.614e+01\n",
      "[2020-12-17 03:52:12] Step 65, trainingset loss = 56.139, first 4 params = 0.20 0.36 1.53 1.81\n",
      "[2020-12-17 03:53:34] Step 66, trainingset loss = 134.209, first 4 params = 0.10 0.24 1.54 1.94\n",
      "[2020-12-17 03:54:58] Step 67, trainingset loss = 68.641, first 4 params = 0.17 0.34 1.54 1.83\n",
      "[2020-12-17 03:56:49] Step 68, trainingset loss = 59.629, first 4 params = 0.31 0.49 1.51 1.67\n",
      "[2020-12-17 03:58:37] Best trainingset loss = 5.235e+01\n",
      "[2020-12-17 03:58:37] Step 69, trainingset loss = 52.352, first 4 params = 0.19 0.32 1.52 1.85\n",
      "[2020-12-17 04:00:05] Step 70, trainingset loss = 66.145, first 4 params = 0.12 0.22 1.53 1.97\n",
      "[2020-12-17 04:00:05] Time per f-evaluation (trainingset): 0:01:43.156113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-17 04:01:30] Step 71, trainingset loss = 65.044, first 4 params = 0.15 0.29 1.53 1.89\n",
      "[2020-12-17 04:03:28] Step 72, trainingset loss = 57.206, first 4 params = 0.27 0.44 1.51 1.72\n",
      "[2020-12-17 04:05:16] Best trainingset loss = 5.101e+01\n",
      "[2020-12-17 04:05:16] Step 73, trainingset loss = 51.006, first 4 params = 0.18 0.29 1.50 1.91\n",
      "[2020-12-17 04:07:01] Step 74, trainingset loss = 53.082, first 4 params = 0.13 0.20 1.48 2.02\n",
      "[2020-12-17 04:08:29] Step 75, trainingset loss = 58.942, first 4 params = 0.14 0.25 1.51 1.95\n",
      "[2020-12-17 04:10:20] Step 76, trainingset loss = 55.078, first 4 params = 0.24 0.39 1.51 1.78\n",
      "[2020-12-17 04:11:47] Step 77, trainingset loss = 62.169, first 4 params = 0.15 0.29 1.53 1.89\n",
      "[2020-12-17 04:13:38] Step 78, trainingset loss = 54.377, first 4 params = 0.23 0.37 1.51 1.81\n",
      "[2020-12-17 04:15:33] Step 79, trainingset loss = 58.132, first 4 params = 0.22 0.32 1.49 1.87\n",
      "[2020-12-17 04:17:22] Step 80, trainingset loss = 53.737, first 4 params = 0.21 0.35 1.52 1.82\n",
      "[2020-12-17 04:17:22] Time per f-evaluation (trainingset): 0:01:43.222818\n",
      "[2020-12-17 04:19:14] Step 81, trainingset loss = 54.055, first 4 params = 0.16 0.28 1.51 1.92\n",
      "[2020-12-17 04:20:45] Step 82, trainingset loss = 57.251, first 4 params = 0.14 0.25 1.52 1.94\n",
      "[2020-12-17 04:22:35] Step 83, trainingset loss = 52.836, first 4 params = 0.21 0.34 1.51 1.84\n",
      "[2020-12-17 04:24:25] Step 84, trainingset loss = 54.368, first 4 params = 0.23 0.38 1.51 1.80\n",
      "[2020-12-17 04:26:11] Step 85, trainingset loss = 51.507, first 4 params = 0.18 0.30 1.51 1.89\n",
      "[2020-12-17 04:27:58] Best trainingset loss = 5.064e+01\n",
      "[2020-12-17 04:27:58] Step 86, trainingset loss = 50.636, first 4 params = 0.17 0.27 1.50 1.92\n",
      "[2020-12-17 04:29:48] Best trainingset loss = 4.977e+01\n",
      "[2020-12-17 04:29:48] Step 87, trainingset loss = 49.771, first 4 params = 0.16 0.23 1.49 1.97\n",
      "[2020-12-17 04:31:38] Step 88, trainingset loss = 52.542, first 4 params = 0.15 0.23 1.50 1.97\n",
      "[2020-12-17 04:33:20] Best trainingset loss = 4.903e+01\n",
      "[2020-12-17 04:33:20] Step 89, trainingset loss = 49.029, first 4 params = 0.16 0.26 1.50 1.94\n",
      "[2020-12-17 04:35:23] Step 90, trainingset loss = 50.745, first 4 params = 0.15 0.22 1.48 2.00\n",
      "[2020-12-17 04:35:23] Time per f-evaluation (trainingset): 0:01:43.776005\n",
      "[2020-12-17 04:37:10] Best trainingset loss = 4.718e+01\n",
      "[2020-12-17 04:37:10] Step 91, trainingset loss = 47.179, first 4 params = 0.14 0.20 1.48 2.02\n",
      "[2020-12-17 04:39:13] Step 92, trainingset loss = 51.067, first 4 params = 0.12 0.15 1.46 2.09\n",
      "[2020-12-17 04:41:04] Step 93, trainingset loss = 51.309, first 4 params = 0.12 0.17 1.48 2.06\n",
      "[2020-12-17 04:42:52] Step 94, trainingset loss = 50.150, first 4 params = 0.17 0.26 1.49 1.94\n",
      "[2020-12-17 04:44:39] Step 95, trainingset loss = 50.403, first 4 params = 0.17 0.26 1.50 1.94\n",
      "[2020-12-17 04:46:26] Step 96, trainingset loss = 50.005, first 4 params = 0.16 0.25 1.50 1.95\n",
      "[2020-12-17 04:48:14] Step 97, trainingset loss = 47.350, first 4 params = 0.15 0.21 1.49 2.00\n",
      "[2020-12-17 04:50:17] Step 98, trainingset loss = 50.234, first 4 params = 0.14 0.20 1.48 2.01\n",
      "[2020-12-17 04:51:59] Step 99, trainingset loss = 49.377, first 4 params = 0.16 0.24 1.49 1.97\n",
      "[2020-12-17 04:54:05] Step 100, trainingset loss = 50.288, first 4 params = 0.15 0.22 1.49 1.99\n",
      "[2020-12-17 04:54:05] Time per f-evaluation (trainingset): 0:01:44.626755\n",
      "[2020-12-17 04:55:49] Step 101, trainingset loss = 49.220, first 4 params = 0.15 0.23 1.49 1.98\n",
      "[2020-12-17 04:57:42] Step 102, trainingset loss = 47.331, first 4 params = 0.15 0.21 1.49 2.00\n",
      "[2020-12-17 04:59:44] Step 103, trainingset loss = 50.591, first 4 params = 0.14 0.21 1.49 2.00\n",
      "[2020-12-17 05:01:27] Step 104, trainingset loss = 49.182, first 4 params = 0.15 0.23 1.49 1.98\n",
      "[2020-12-17 05:03:18] Step 105, trainingset loss = 47.462, first 4 params = 0.15 0.22 1.49 1.99\n",
      "[2020-12-17 05:05:21] Step 106, trainingset loss = 50.417, first 4 params = 0.13 0.16 1.47 2.07\n",
      "[2020-12-17 05:07:05] Step 107, trainingset loss = 48.050, first 4 params = 0.15 0.24 1.49 1.97\n",
      "[2020-12-17 05:08:59] Best trainingset loss = 4.686e+01\n",
      "[2020-12-17 05:08:59] Step 108, trainingset loss = 46.855, first 4 params = 0.14 0.19 1.48 2.04\n",
      "[2020-12-17 05:11:02] Step 109, trainingset loss = 50.417, first 4 params = 0.13 0.16 1.47 2.07\n",
      "[2020-12-17 05:12:49] Step 110, trainingset loss = 47.079, first 4 params = 0.14 0.19 1.48 2.03\n",
      "[2020-12-17 05:12:49] Time per f-evaluation (trainingset): 0:01:46.138108\n",
      "[2020-12-17 05:14:43] Step 111, trainingset loss = 46.892, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:16:32] Step 112, trainingset loss = 47.186, first 4 params = 0.13 0.16 1.46 2.07\n",
      "[2020-12-17 05:18:23] Step 113, trainingset loss = 47.126, first 4 params = 0.13 0.18 1.47 2.05\n",
      "[2020-12-17 05:20:17] Step 114, trainingset loss = 46.946, first 4 params = 0.13 0.17 1.47 2.06\n",
      "[2020-12-17 05:22:12] Step 115, trainingset loss = 47.013, first 4 params = 0.14 0.19 1.48 2.04\n",
      "[2020-12-17 05:24:15] Step 116, trainingset loss = 50.809, first 4 params = 0.13 0.17 1.47 2.06\n",
      "[2020-12-17 05:26:06] Step 117, trainingset loss = 47.169, first 4 params = 0.14 0.18 1.48 2.04\n",
      "[2020-12-17 05:28:01] Best trainingset loss = 4.683e+01\n",
      "[2020-12-17 05:28:01] Step 118, trainingset loss = 46.831, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:29:54] Step 119, trainingset loss = 46.884, first 4 params = 0.13 0.18 1.47 2.05\n",
      "[2020-12-17 05:31:48] Step 120, trainingset loss = 46.930, first 4 params = 0.14 0.19 1.48 2.04\n",
      "[2020-12-17 05:31:48] Time per f-evaluation (trainingset): 0:01:47.091531\n",
      "[2020-12-17 05:33:41] Step 121, trainingset loss = 47.171, first 4 params = 0.14 0.19 1.48 2.03\n",
      "[2020-12-17 05:35:44] Step 122, trainingset loss = 50.242, first 4 params = 0.13 0.18 1.47 2.05\n",
      "[2020-12-17 05:37:31] Step 123, trainingset loss = 47.222, first 4 params = 0.14 0.18 1.48 2.04\n",
      "[2020-12-17 05:39:26] Step 124, trainingset loss = 46.865, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:41:19] Step 125, trainingset loss = 46.873, first 4 params = 0.13 0.18 1.47 2.04\n",
      "[2020-12-17 05:43:12] Step 126, trainingset loss = 46.905, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:44:59] Step 127, trainingset loss = 47.221, first 4 params = 0.14 0.19 1.47 2.04\n",
      "[2020-12-17 05:46:53] Step 128, trainingset loss = 47.048, first 4 params = 0.13 0.18 1.47 2.04\n",
      "[2020-12-17 05:48:48] Step 129, trainingset loss = 46.961, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:50:41] Best trainingset loss = 4.680e+01\n",
      "[2020-12-17 05:50:41] Step 130, trainingset loss = 46.796, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:50:41] Time per f-evaluation (trainingset): 0:01:47.875605\n",
      "[2020-12-17 05:52:28] Step 131, trainingset loss = 47.221, first 4 params = 0.14 0.19 1.47 2.04\n",
      "[2020-12-17 05:54:22] Step 132, trainingset loss = 46.797, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 05:56:17] Step 133, trainingset loss = 46.803, first 4 params = 0.14 0.19 1.47 2.04\n",
      "[2020-12-17 05:58:11] Best trainingset loss = 4.677e+01\n",
      "[2020-12-17 05:58:11] Step 134, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:00:00] Step 135, trainingset loss = 47.171, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:01:47] Step 136, trainingset loss = 47.216, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:03:41] Step 137, trainingset loss = 46.831, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:05:34] Step 138, trainingset loss = 46.781, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:07:29] Step 139, trainingset loss = 46.781, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:09:22] Step 140, trainingset loss = 46.783, first 4 params = 0.14 0.19 1.47 2.04\n",
      "[2020-12-17 06:09:22] Time per f-evaluation (trainingset): 0:01:48.278973\n",
      "[2020-12-17 06:11:16] Step 141, trainingset loss = 46.819, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:13:05] Step 142, trainingset loss = 47.225, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:15:00] Best trainingset loss = 4.676e+01\n",
      "[2020-12-17 06:15:00] Step 143, trainingset loss = 46.756, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:17:20] Step 144, trainingset loss = 46.785, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:19:18] Step 145, trainingset loss = 46.781, first 4 params = 0.14 0.18 1.47 2.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-17 06:21:11] Step 146, trainingset loss = 46.782, first 4 params = 0.14 0.19 1.47 2.04\n",
      "[2020-12-17 06:23:08] Best trainingset loss = 4.674e+01\n",
      "[2020-12-17 06:23:08] Step 147, trainingset loss = 46.740, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:25:00] Step 148, trainingset loss = 46.784, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:26:54] Step 149, trainingset loss = 46.782, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:28:48] Step 150, trainingset loss = 46.789, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:28:48] Time per f-evaluation (trainingset): 0:01:49.945571\n",
      "[2020-12-17 06:30:40] Step 151, trainingset loss = 46.775, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:32:33] Step 152, trainingset loss = 46.782, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:34:28] Step 153, trainingset loss = 46.780, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:36:22] Step 154, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:38:15] Step 155, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:40:10] Step 156, trainingset loss = 46.768, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:42:04] Step 157, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:43:57] Step 158, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:45:53] Best trainingset loss = 4.673e+01\n",
      "[2020-12-17 06:45:53] Step 159, trainingset loss = 46.730, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:47:46] Step 160, trainingset loss = 46.775, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:47:46] Time per f-evaluation (trainingset): 0:01:50.546887\n",
      "[2020-12-17 06:49:41] Step 161, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:51:34] Step 162, trainingset loss = 46.778, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:53:28] Step 163, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:55:23] Step 164, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:57:17] Step 165, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 06:59:10] Step 166, trainingset loss = 46.766, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:01:04] Step 167, trainingset loss = 46.768, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:02:58] Step 168, trainingset loss = 46.767, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:04:52] Step 169, trainingset loss = 46.767, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:06:45] Step 170, trainingset loss = 46.767, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:06:45] Time per f-evaluation (trainingset): 0:01:52.001148\n",
      "[2020-12-17 07:08:40] Step 171, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:10:33] Step 172, trainingset loss = 46.768, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:12:26] Step 173, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:14:21] Step 174, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:16:13] Step 175, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:18:07] Step 176, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:20:01] Step 177, trainingset loss = 46.769, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:21:55] Step 178, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:23:48] Step 179, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:25:41] Step 180, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:25:41] Time per f-evaluation (trainingset): 0:01:52.994339\n",
      "[2020-12-17 07:27:34] Step 181, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:29:28] Step 182, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:31:22] Step 183, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:33:15] Step 184, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:35:10] Step 185, trainingset loss = 46.730, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:37:05] Step 186, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:38:58] Step 187, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:40:52] Step 188, trainingset loss = 46.773, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:42:47] Best trainingset loss = 4.673e+01\n",
      "[2020-12-17 07:42:47] Step 189, trainingset loss = 46.730, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:44:39] Step 190, trainingset loss = 46.773, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:44:39] Time per f-evaluation (trainingset): 0:01:53.563355\n",
      "[2020-12-17 07:46:35] Best trainingset loss = 4.673e+01\n",
      "[2020-12-17 07:46:35] Step 191, trainingset loss = 46.729, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:48:30] Step 192, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:50:24] Step 193, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:52:17] Step 194, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:54:10] Step 195, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:56:04] Step 196, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:57:57] Step 197, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 07:59:51] Step 198, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:01:45] Step 199, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:03:39] Step 200, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:03:39] Time per f-evaluation (trainingset): 0:01:53.739221\n",
      "[2020-12-17 08:05:32] Step 201, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:07:26] Step 202, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:09:20] Step 203, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:11:14] Step 204, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:13:08] Step 205, trainingset loss = 46.773, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:15:02] Step 206, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:16:57] Step 207, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:18:50] Step 208, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:20:44] Step 209, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:22:37] Step 210, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:22:37] Time per f-evaluation (trainingset): 0:01:53.882924\n",
      "[2020-12-17 08:24:31] Step 211, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:26:24] Step 212, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:28:19] Step 213, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:30:13] Step 214, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:32:07] Step 215, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:34:01] Step 216, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:35:56] Step 217, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:37:48] Step 218, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:39:42] Step 219, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:41:35] Step 220, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:41:35] Time per f-evaluation (trainingset): 0:01:53.867024\n",
      "[2020-12-17 08:43:29] Step 221, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:45:23] Step 222, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-12-17 08:47:16] Step 223, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:49:09] Step 224, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:51:03] Step 225, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:52:56] Step 226, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:54:50] Step 227, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:56:45] Step 228, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 08:58:38] Step 229, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:00:32] Step 230, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:00:32] Time per f-evaluation (trainingset): 0:01:53.912672\n",
      "[2020-12-17 09:02:26] Step 231, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:04:21] Step 232, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:06:15] Step 233, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:08:10] Step 234, trainingset loss = 46.774, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:10:04] Step 235, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:11:57] Step 236, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:13:50] Step 237, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:15:44] Step 238, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:17:38] Step 239, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:19:32] Step 240, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:19:32] Time per f-evaluation (trainingset): 0:01:54.103242\n",
      "[2020-12-17 09:21:24] Step 241, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:23:19] Step 242, trainingset loss = 46.732, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:25:14] Step 243, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:27:06] Step 244, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:28:59] Step 245, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:30:53] Step 246, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:32:47] Step 247, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:34:40] Step 248, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:36:33] Step 249, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:38:26] Step 250, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:38:26] Time per f-evaluation (trainingset): 0:01:53.785494\n",
      "[2020-12-17 09:40:20] Step 251, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:42:13] Step 252, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:44:05] Step 253, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:45:59] Step 254, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:47:52] Step 255, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:49:45] Step 256, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:51:37] Step 257, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:53:30] Step 258, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:55:24] Step 259, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:57:18] Step 260, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 09:57:18] Time per f-evaluation (trainingset): 0:01:53.715430\n",
      "[2020-12-17 09:59:11] Step 261, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:01:04] Step 262, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:02:57] Step 263, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:04:52] Step 264, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:06:44] Step 265, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:08:38] Step 266, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:10:32] Step 267, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:12:26] Step 268, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:14:19] Step 269, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:16:13] Step 270, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:16:13] Time per f-evaluation (trainingset): 0:01:53.685515\n",
      "[2020-12-17 10:18:06] Step 271, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:20:01] Step 272, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:21:54] Step 273, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:23:48] Step 274, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:25:42] Step 275, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:27:36] Step 276, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:29:29] Step 277, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:31:24] Step 278, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:33:17] Step 279, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:35:11] Step 280, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:35:11] Time per f-evaluation (trainingset): 0:01:53.698108\n",
      "[2020-12-17 10:37:04] Step 281, trainingset loss = 46.770, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:39:46] Step 282, trainingset loss = 46.772, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:44:25] Step 283, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n",
      "[2020-12-17 10:49:30] Step 284, trainingset loss = 46.771, first 4 params = 0.14 0.18 1.47 2.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5daa08ece126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0moptimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/parameteroptimization.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mr\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[0;31m# Scaler determines the bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mf\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0m_Step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# make one callable function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparametervectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled2real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/optimizers/scipy.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, function, x0, bounds, workers)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMinimizeResult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mr\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mspmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scipy finished with message: {r.message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    587\u001b[0m                       callback=callback, **options)\n\u001b[1;32m    588\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nelder-mead'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_neldermead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'powell'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_powell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, **unknown_options)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mxbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxbar\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mfxr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         \u001b[0mdoshrink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, workers, full, _force)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_force\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# partial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m_atom\u001b[0;34m(self, x, full, _force)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mfx\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Logging for training set only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled2real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# skip evaluation: fx==nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled2real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, engine, parallel)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pipe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_skip_normjobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_residuals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/params/core/opt_components.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pipe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_skip_normjobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresiduals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_residuals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/scripting/scm/plams/core/results.py\u001b[0m in \u001b[0;36mguardian\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hellstrom/adfhome/bin/python3.6/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = Scipy(method='Nelder-Mead')\n",
    "\n",
    "optimization = Optimization(jc, \n",
    "                            training_set, \n",
    "                            interface, \n",
    "                            optimizer, \n",
    "                            title=\"ZnO_repulsive_opt\",\n",
    "                            use_pipe=False, \n",
    "                            parallel=ParallelLevels(processes=num_processes), \n",
    "                            #plams_workdir_path=os.path.abspath('.'),\n",
    "                            callbacks=[Logger(printfreq=1,\n",
    "                                              writefreq_history=1,\n",
    "                                              writefreq_datafiles=1,\n",
    "                                              writefreq_bestparams=1\n",
    "                                             ),\n",
    "                                      TimePerEval(printfrequency=10)])\n",
    "\n",
    "\n",
    "optimization.summary()\n",
    "optimization.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Find the results\n",
    "* ZnO_repulsive_opt/trainingset_history.dat contains the loss function value and parameters for each iteration\n",
    "* ZnO_repulsive_opt/data/predictions/trainingset contains the individual predictions ($a$, $c$, $B_0$ and $\\Delta E$) for each parameter set\n",
    "* ZnO_repulsive_opt/data/contributions/trainingset contains the fraction of the total loss function value for each item in the training set, for each parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Step 2b: Recalculate the reference data with AMS BAND\n",
    "Any engine, or combination of different engines in the Amsterdam Modeling Suite, can be used to seamlessly calculate the reference data, if the reference values are not known beforehand.\n",
    "\n",
    "Here, we will use the AMS BAND periodic DFT code. For this demonstration, we will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `range` attribute allows to define box constraints for every optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
